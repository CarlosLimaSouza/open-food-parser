# Open Food Facts Parser API

Uma API RESTful desenvolvida para facilitar a curadoria e revis√£o de dados nutricionais do projeto Open Food Facts pela equipe da Fitness Foods LC.

Este √© um desafio t√©cnico (challenge) da [Coodesh](https://coodesh.com/).

---

## Como eu vou resolver isso: Meu plano

Esse aqui √© o meu roteiro (e a documenta√ß√£o da minha linha de racioc√≠nio). Vou seguir as regras usando a stack mais proxima possivel da vaga. Pode ser que eu tenha que revisar o plano depois, mas j√° √© bom ter algo para usar como uma esp√©cie de fluxograma de como vou abordar o problema.

---

### A Stack que eu escolhi
- **Linguagem:** PHP 8.3
- **Framework:** Laravel 11
- **Banco de Dados:** PostgreSQL 16
- **Cache/Fila:** Redis (Para eu n√£o travar o CRON)
- **Container:** Docker com Laravel Sail (para eu n√£o ter dor de cabe√ßa com ambiente)
- **Testes:** PHPUnit

---

### üìù O que eu vou fazer (Passo a Passo)

#### Passo 1: Preparando o meu terreno (Setup)
Antes de come√ßar a codar, eu preciso deixar o ambiente pronto.
- **Minha ideia:** Vou usar o Docker para garantir que tudo funcione igual no meu PC e no deploy.
- **Detalhe:** Vou configurar o Laravel para j√° reconhecer o banco e avisar que o Redis vai cuidar das filas pesadas que eu vou criar depois.

#### Passo 2: Como eu vou guardar esses dados?
Agora eu foco no banco. Preciso criar uma estrutura que aceite o JSON deles, mas com os meus campos extras: quero saber a hora exata que importei e o status (se est√° em rascunho, publicado ou no lixo).
Preciso configurar uma tabela de logs , registra cada tentativa de importa√ß√£o (se deu problema, quantos itens vieram,etc).
- **Lembrete:** N√£o vou deletar nada de verdade agora. Se eu precisar apagar algo, s√≥ mudo o status para "trash".
- **Para depois:** Se esse banco explodisse de tamanho, como eu iria manter a busca r√°pida? Por enquanto, vou manter simples com √≠ndices b√°sicos, mas √© algo a se pensar.

#### Passo 3: Encarando a importa√ß√£o (A parte chata)
Aqui √© o maior desafio. Vou ter que baixar arquivos gigantes todo dia.
Eu fiz um teste inicial e vi que o arquivo compactado j√° passa dos 55MB, o que significa que descompactado ele deve bater quase 1GB de puro texto.
- **Minha estrat√©gia:** N√£o vou tentar ler tudo de uma vez para n√£o fritar o servidor. Vou ler em peda√ßos e salvar s√≥ os primeiros 100 de cada arquivo.
- **Meu Plano B:** Vou deixar para detalhar as filas e as tentativas de erro quando eu estiver com a m√£o na massa, porque sei que isso ai pode complicar e aparecer novas necessidade que n√£o pensei agora. 
Por enquanto o que eu ja sei √© que se o site deles cair ou o download falhar, vou usar o sistema de logs que planejei no Passo 2 para me avisar. Se der erro, o Job volta para a fila para tentar de novo (retry), assim n√£o perco o dado.

#### Passo 4: Expondo meus dados (Criando a API)
Vou fazer o b√°sico:
- Uma rota inicial s√≥ para eu checar a sa√∫de do sistema.
- A lista de produtos (com pagina√ß√£o, porque al√©m de boas pr√°ticas e eu n√£o sou maluco de carregar tudo de uma vez).
- Os jeitos de eu ver, editar e "esconder" cada produto.
- **Lembrete:** Usar o SKU do produto e n√£o o ID autoincrement nas rotas , j√° que o desafio foca no campo 'code'.

#### Passo 5: Seguran√ßa e o meu manual
Vou colocar uma chave (API Key) na porta de entrada. 
- Vou criar o manual da API usando o padr√£o OpenAPI 3.0 (Swagger). √â um diferencial da vaga (e de quebra, j√° deixo documentado como usar a chave de seguran√ßa).

#### Passo 6: Check-up final (Ser√° que funcionou?)
Antes de dar como finalizado, vou testar tudo. Vou ver se minha chave bloqueia intrusos, se o produto realmente vai para o lixo quando eu mando e se os dados est√£o consistentes.
- Se eu encontrar erro, eu paro, respiro (vou passear com os cachorros) e conserto. O foco √© eu entregar um n√∫cleo s√≥lido, e o prazo √© mais que o suficiente para n√£o me afobar.

---

### ‚úÖ Meu Checklist
- [ ] Docker rodando redondo com Postgres e Redis.
- [ ] Estrutura do banco seguindo o modelo que eu planejei.
- [ ] Meu CRON trabalhando em sil√™ncio via filas.
- [ ] Limite de 100 itens sendo respeitado (n√£o posso esquecer!).
- [ ] Minha API respondendo JSON bonitinho.
- [ ] Todos os meus testes passando com um `php artisan test`.

--- 

## üöÄ Como instalar e usar 

**Pr√©-requisitos:** Docker Desktop instalado.

1. **Clone o reposit√≥rio:**
   ```bash
   git clone https://github.com/CarlosLimaSouza/open-food-parser.git
   cd open-food-parser
   ```

2. **Suba o ambiente (Docker):**
   ```bash
   ./vendor/bin/sail up -d
   ```

3. **Instale as depend√™ncias:**
   ```bash
   ./vendor/bin/sail composer install
   ```

4. **Prepare o Banco de Dados:**
   ```bash
   ./vendor/bin/sail artisan migrate
   ```

5. **Acesse a API:**
   - Documenta√ß√£o (Swagger): `http://localhost/api/documentation`
   - Testes: `./vendor/bin/sail artisan test`

---
